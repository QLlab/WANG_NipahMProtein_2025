# -*- coding: utf-8 -*-
"""K_stage_from_rawdata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WV1KLU6mWKVxKoSA914GUKNeDaWYZMEW
"""

import pandas as pd
import numpy as np
from scipy.signal import savgol_filter
import os
import re
from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/livecells/many2/

# only used for merging all data without filter

csv_folder = '/content/drive/MyDrive/livecells/many2/'
csv_files = [os.path.join(csv_folder, f) for f in os.listdir(csv_folder) if f.endswith('.csv') and 'all' not in f]

# Create a list to store all processed DataFrames to improve performance.
all_dfs = []

for file in csv_files:
    df = pd.read_csv(file, skiprows=3)
    df = df.loc[:, ~df.columns.str.contains("Unnamed")]
    # Get the number from the filename.
    file_number = file.split("_")[1]  # Assuming the number is in position 1 of the filename.

    # Create a dictionary to store the new columns.
    new_columns = {}

    # Process other columns
    for col in df.columns:
        if col.startswith('Intensity Sum') and col.endswith('::0'):
            # extract number
            number = re.search(r"::(\d+)::", col).group(1)

            # new column name
            new_column_name = f"{file_number}_{number}"
            new_columns[new_column_name] = df[col]

    # Create a new DataFrame using a dictionary.
    new_df = pd.DataFrame(new_columns)
    all_dfs.append(new_df)

# Use pd.concat to merge all DataFrames at once.
all_df = pd.concat(all_dfs, axis=1)

# Add time column
all_df['time'] = range(0, len(all_df) * 5, 5)
# Rearrange the order of the columns to ensure that the time column is the first column.
cols = ['time'] + [col for col in all_df.columns if (col != 'time')]
all_df = all_df[cols]

# Write the merged DataFrame to a new CSV file.
all_df.to_csv("all_output.csv", index=False)

all_df.head()

to_filter_df = all_df.iloc[:, 2:]
time_index = all_df.iloc[:,0].astype(int)
time_index.columns = ["time"]

column_indices = []

# Filling in the missing values in the middle.
for col in to_filter_df.columns:
    # Obtain the original length of this column.
    col_length = len(to_filter_df[col].dropna())
    # print(col_length)

    if(col_length==0): # Prevent an entire row from being empty.
        print("Completely empty")
        continue

    # Calculate the number of consecutive empty values at the beginning
    # Get the index of the first non-NaN value in this column
    first_not_nan = to_filter_df[col].first_valid_index()

    # If there are values that are not NaN.
    if first_not_nan is not None:
        # Calculate the interval from the beginning to the first non-NaN value.
        interval = first_not_nan
        # print(f"The column '{col}' has an interval of {interval} rows from the beginning to the first non-NaN value.")
    else:
        interval = 0
        print(f"Column '{col}' are all NaN")

    #print(to_filter_df[col])
    # move all values forward
    to_filter_df[col] = to_filter_df[col].shift(-interval, fill_value=pd.NA)

    # fill only the values within the original length
    to_filter_df[col] = to_filter_df[col].ffill().bfill()
    to_filter_df[col] = to_filter_df[col][:col_length]

filtered_df1 = pd.concat([all_df.iloc[:, 0], all_df.iloc[:, 1]], axis=1)
filtered_df = pd.concat([filtered_df1, to_filter_df], axis=1)
filtered_df.to_csv("all_output_filtered.csv", index=False)

filtered_df.head()

# calculate dwelling time
import pandas as pd
import numpy as np
from scipy.signal import savgol_filter
from scipy.interpolate import UnivariateSpline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import os
from scipy.signal import argrelextrema
from sklearn.preprocessing import MinMaxScaler
k_up = 1
k_down = -1



df = filtered_df.drop(filtered_df.columns[0], axis=1)
df = df.iloc[:, :]
print(df.head())
# automatically find the dwelling stage in a curve
# step 1: smooth the curve
results_table = pd.DataFrame(columns=['file', 'col_name', 'k', 'top_start', 'top_end'])

for col in df.columns:
    clean_col = df[col].dropna()
    print(clean_col)
    x = np.arange(len(clean_col))
    y = clean_col

    scaler = MinMaxScaler(feature_range=(0, 1))
    y = scaler.fit_transform(y.values.reshape(-1, 1)).fl  atten()
    y_max = 1
    y_min = 0
    # y_max = y.max()
    # y_min = y.min()
    x_max = x.max()
    # print("y_max ", y_max)
    # print("y_min ", y_min)
    # print("x_max ", x_max)

    # reshape x to a 2D array
    X = x.reshape(-1, 1)

    # create a cubic polynomial regression model
    model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())

    # fit the model
    model.fit(X, y)

    # generate a smooth prediction curve
    X_smooth = np.linspace(x.min(), x.max(), 500).reshape(-1, 1)
    y_smooth = model.predict(X_smooth)

    # get polynomial coefficients
    coefficients = model.named_steps['linearregression'].coef_
    intercept = model.named_steps['linearregression'].intercept_

    # define cubic polynomial function
    def cubic_function(x):
        return coefficients[3] * x ** 3 + coefficients[2] * x ** 2 + coefficients[1] * x + intercept

    # define derivative function
    def derivative(x):
        return 3 * coefficients[3] * x ** 2 + 2 * coefficients[2] * x + coefficients[1]

    # define second derivative function
    def second_derivative(x):
        return 6 * coefficients[3] * x + 2 * coefficients[2]

    # find the top region
    def find_top_region(x_range, slope_min, slope_max):
        x_values = np.linspace(x_range[0], x_range[1], 1000)
        slopes = derivative(x_values)
        curvatures = second_derivative(x_values)

        # find the region where the slope is within the specified range and the curvature is negative
        top_region = x_values[(slopes >= slope_min) & (slopes <= slope_max) & (curvatures < 0)]

        if len(top_region) > 0:
            return top_region.min(), top_region.max()
        else:
            return None, None

    # find x by k
    slope_min, slope_max = k_down * (y_max - y_min) / x_max / 5, k_up * (y_max - y_min) / x_max /5 # x 的原始数据是每个间隔代表5个时间单元
    x_range = (x.min(), x.max())

    # find top
    top_start, top_end = find_top_region(x_range, slope_min, slope_max)

    # ensure the entry point is before the exit point
    if top_start is not None and top_end is not None:
        if top_start > top_end:
            top_start, top_end = top_end, top_start

    # filter small fluctuations
    min_width = 0.1 * (x_range[1] - x_range[0])
    if top_start is not None and top_end is not None:
        if top_end - top_start < min_width:
            top_start, top_end = None, None

    # find k in climbing area
    if top_start is not None and top_end is not None:
        k_xaxis_1_4 = top_start / 4
        k_xaxis_2_4 = top_start / 2
        k_xaxis_3_4 = top_start * 3 / 4
        k_xaxis_4_4 = top_start
        k_xaxis = [k_xaxis_1_4, k_xaxis_2_4, k_xaxis_3_4, k_xaxis_4_4]
        climbing_k_all = []
        for k in k_xaxis:
            climbing_k_all.append(derivative(k))
        climbing_k = np.mean(climbing_k_all)
    else:
        climbing_k = None

    # print results and store to results table
    if climbing_k is not None and top_start is not None and top_end is not None:
        print(f"k: {climbing_k}")
        print(f"The x range of the top region: {top_start:.2f} to {top_end:.2f}")
        print(f"Climbing K value: {climbing_k}")
        results_table.loc[len(results_table)] = [col.split('_')[0], col.split('_')[1], climbing_k, top_start, top_end]
    else:
        print(f"Column {col} did not find a valid peak region")
        results_table.loc[len(results_table)] = [col.split('_')[0], col.split('_')[1], None, None, None]

    # plot
    plt.figure(figsize=(12, 6))
    plt.scatter(x, y, color='blue', alpha=0.5, label='Original data')

    x_smooth = np.linspace(x.min(), x.max(), 500)
    y_smooth = cubic_function(x_smooth)
    plt.plot(x_smooth, y_smooth, color='red', label='Cubic fit')

    if top_start is not None and top_end is not None:
        plt.axvline(x=top_start, color='green', linestyle='--', label='Top region start')
        plt.axvline(x=top_end, color='green', linestyle='--', label='Top region end')
        plt.fill_between(x_smooth, y_smooth, where=(x_smooth >= top_start) & (x_smooth <= top_end), color='yellow', alpha=0.3, label='Top region')

    plt.legend()
    plt.title('Cubic Fit with Top Region Highlighted')
    plt.xlabel('X')
    plt.ylabel('Y')

    # adjust y-axis range
    y_min, y_max = np.min(y), np.max(y)
    y_range = y_max - y_min
    plt.ylim(y_min - 0.05 * y_range, y_max + 0.05 * y_range)

    # set y-axis format
    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))

    plt.tight_layout()
    plt.savefig("/content/drive/MyDrive/livecells/many2/results/" + col + ".png")
    plt.close()

results_table.to_csv('results.txt', sep='\t')

